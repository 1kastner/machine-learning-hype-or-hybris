{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tennisspielen bei verschiedenem Wetter\n",
    "\n",
    "Dies ist der gleiche Datensatz wie bei der Einführung.\n",
    "Diesmal ist der Schwerpunkt auf den verschiedenen Metriken, mit denen die Güte bewertet werden kann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn.tree\n",
    "import sklearn.model_selection\n",
    "import sklearn.metrics\n",
    "\n",
    "df = pd.read_csv(\"../../01 einfuehrende-beispiele/tennis.tsv\", sep=\" \\t\", engine=\"python\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "one_hot_encoded = pd.get_dummies(df[[\"Outlook\", \"Temperature\", \"Humidity\", \"Wind\"]])\n",
    "one_hot_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(one_hot_encoded, df[\"Play Tennis?\"])\n",
    "X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = sklearn.tree.DecisionTreeClassifier()\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred = dt.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun visualisieren wir einmal als Tabelle kurz, wie häufig wir falsch lagen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data={\n",
    "    \"predicted\": y_pred,\n",
    "    \"actual\": y_test\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metriken\n",
    "\n",
    "Die Metriken helfen dabei, in Zahlen die Diskrepanz zwischen der Vorhersage und dem tatsächlichen Wert in Zahlen zu fassen.\n",
    "Hier nun ein paar Beispiele, was man messen kann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Metriken\")\n",
    "for entry in dir(sklearn.metrics):\n",
    "    if entry.startswith(\"_\") or entry == entry.upper():\n",
    "        continue\n",
    "    print(\"-\", entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manche der Metriken sind für Regressionsprobleme, manche für Klassifizierungsprobleme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(y_pred), type(y_test.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion `sklearn.metrics.mean_absolute_error` berechnet den Betrag der Differenz zwischen der Vorhersage und dem tatsächlichen Wert.\n",
    "Von all diesen Differenzen wird der Mittelwert gebildet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.mean_absolute_error(y_pred, y_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?sklearn.metrics.precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, fscore, support = sklearn.metrics.precision_recall_fscore_support(y_test.values, y_pred,\n",
    "                                                                                     labels=[\"Yes\", \"No\"])\n",
    "\n",
    "print(\"Precision \\t\", precision)\n",
    "print(\"Recall \\t\\t\", recall)\n",
    "print(\"F-Score \\t\", fscore)\n",
    "print(\"Support \\t\", support)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gerade bei selten auftretenden Ereignissen ist die Größe \"Recall\" sehr interessant.\n",
    "\n",
    "$Recall = \\frac{TP}{TP + FN}$\n",
    "\n",
    "TP: True Positive, sprich der echte positive Fall\n",
    "\n",
    "FN: False Negative, sprich ein eigentlich positiver Fall, der aber fälschlicherweise als negativ eingestuft wurde.\n",
    "\n",
    "Bei ärztlichen Diagnosen entspricht dies der Fähigkeit, dass eine Diagnose bei allen positiven Fällen ausschlägt."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml-hype-or-hybris] *",
   "language": "python",
   "name": "conda-env-ml-hype-or-hybris-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
